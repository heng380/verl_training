# Configuration for Qwen2.5-Math-1.5B GRPO training
# This config extends the default ppo_trainer.yaml and overrides specific parameters

hydra:
  searchpath:
    - file://../../verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Data configuration
data:
  train_files: /home/ubuntu/data/gsm8k/train.parquet
  val_files: /home/ubuntu/data/gsm8k/test.parquet
  train_batch_size: 512  # Increased for 8 GPUs
  max_prompt_length: 512
  max_response_length: 1024

# Actor, rollout and reference model configuration
actor_rollout_ref:
  model:
    path: /home/ubuntu/model/Qwen2.5-0.5B
  
  actor:
    optim:
      lr: 1e-6
    ppo_mini_batch_size: 128  # Increased for larger train_batch_size
    ppo_micro_batch_size_per_gpu: 16  # Increased to better utilize 8 GPUs
    loss_agg_mode: seq-mean-token-sum-norm # GRPO paper's loss aggregation (sequence-level mean of token-level mean)
    fsdp_config:
      model_dtype: bfloat16
  
  rollout:
    name: vllm
    dtype: bfloat16
    log_prob_micro_batch_size_per_gpu: 16  # Increased
    tensor_model_parallel_size: 2  # Use 2 GPUs for tensor parallel in vLLM rollout
    gpu_memory_utilization: 0.8  # Increased to better utilize GPU memory
    n: 4  # Generate 4 responses per prompt for GRPO (group sampling)
  
  ref:
    log_prob_micro_batch_size_per_gpu: 8  # Increased

# Algorithm configuration
algorithm:
  # GRPO advantage estimator (no critic needed)
  adv_estimator: grpo
  kl_ctrl:
    kl_coef: 0.001

# Critic configuration (disabled for GRPO)
critic:
  enable: false

# Reward model configuration
# Note: num_examine is set in main_ppo.py (0 for training, 1 for validation)
# So we don't set it here to avoid duplicate argument error
reward_model:
  reward_kwargs: {}

# Trainer configuration
trainer:
  val_before_train: True
  n_gpus_per_node: 8  # Use all 8 GPUs
  nnodes: 1
  save_freq: 50
  test_freq: 10
  total_epochs: 15
  resume_mode: disable  # Disable checkpoint resuming
  logger: ["console", "wandb"]
  project_name: numina-cot
  experiment_name: drgrpo

